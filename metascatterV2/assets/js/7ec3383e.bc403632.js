"use strict";(self.webpackChunkmetascatter=self.webpackChunkmetascatter||[]).push([[704],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return g}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(a),g=r,m=u["".concat(s,".").concat(g)]||u[g]||p[g]||i;return a?n.createElement(m,o(o({ref:t},d),{},{components:a})):n.createElement(m,o({ref:t},d))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},5050:function(e,t,a){a.r(t),a.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return g},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return p}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],l={sidebar_position:6},s="Example data",c={unversionedId:"getting-started/example-data",id:"getting-started/example-data",title:"Example data",description:"Classification",source:"@site/docs/getting-started/example-data.md",sourceDirName:"getting-started",slug:"/getting-started/example-data",permalink:"/metascatterV2/docs/getting-started/example-data",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Data Loading",permalink:"/metascatterV2/docs/getting-started/data-loading"},next:{title:"Basic usage: Image explorer",permalink:"/metascatterV2/docs/using-metascatter/basic-usage-explorer"}},d={},p=[{value:"Classification",id:"classification",level:2},{value:"Object detection",id:"object-detection",level:2},{value:"Self-supervised learning",id:"self-supervised-learning",level:2},{value:"Dataset References",id:"dataset-references",level:2}],u={toc:p};function g(e){var t=e.components,a=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"example-data"},"Example data"),(0,i.kt)("h2",{id:"classification"},"Classification"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Dog breed images"),": ",(0,i.kt)("a",{parentName:"li",href:"http://vision.stanford.edu/aditya86/ImageNetDogs/"},"Stanford Dog Dataset"),(0,i.kt)("sup",null,"[1][2]")," for classifying (~20k) images into one of 120 breeds using ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1iDUoydBgbmKC0Ieu8MIIksBKXdmTx-E8/view?usp=sharing"},"MobileNet")," ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/11zS7QwCClcOmVxZ0Bb4wHukv2aAQAedD/view?usp=sharing"},"[Download CSV]")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Bone Marrow cell types"),": ",(0,i.kt)("a",{parentName:"li",href:"https://www.kaggle.com/datasets/andrewmvd/bone-marrow-cell-classification"},"Bone Marrow Cell Classification"),(0,i.kt)("sup",null,"[3][4]")," ~170k cells from the bone marrow smears of 945 patients using ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1NffRTzLXIAevpLKc5PR03s69FzGlsHbN/view?usp=sharing"},"AlexNet")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Eye fundus images"),": ",(0,i.kt)("a",{parentName:"li",href:"https://ieee-dataport.org/open-access/retinal-fundus-multi-disease-image-dataset-rfmid"},"RFMiD"),(0,i.kt)("sup",null,"[5]")," and ",(0,i.kt)("a",{parentName:"li",href:"https://ieee-dataport.org/open-access/diabetic-retinopathy-fundus-image-datasetagar300"},"AGAR300"),(0,i.kt)("sup",null,"[6][7]","[8]"),"\nfor the classification of Diabetic Retinopathy ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1WINSgmcCj1cLHaNmlwgLXwV08RI4N2yQ/view?usp=sharing"},"[Download CSV]"))),(0,i.kt)("h2",{id:"object-detection"},"Object detection"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Wheat ear detection"),": ",(0,i.kt)("a",{parentName:"li",href:"https://www.kaggle.com/competitions/global-wheat-detection/data"},"Global Wheat Detection Challenge"),(0,i.kt)("sup",null,"[9]")," using PyTorch (Ultralytics) YOLOv5 ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1qwZR1LBzD6Rjx8CP-1fVIL3XtiMY1Ju6/view?usp=sharing"},"[Download CSV]"))),(0,i.kt)("h2",{id:"self-supervised-learning"},"Self-supervised learning"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Breast histpathology images"),": ",(0,i.kt)("a",{parentName:"li",href:"https://www.kaggle.com/datasets/paultimothymooney/breast-histopathology-images"},"Kaggle Challenge"),(0,i.kt)("sup",null,"[10][11]")," using VISSL ",(0,i.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1mrqhP2hugQBYTIyIdyo-nSROuaw69Taq/view?usp=sharing"},"[Download CSV]"))),(0,i.kt)("h2",{id:"dataset-references"},"Dataset References"),(0,i.kt)("p",null,"[1]"," Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao and Li Fei-Fei. Novel dataset for Fine-Grained Image Categorization. First Workshop on Fine-Grained Visual Categorization (FGVC), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011. "),(0,i.kt)("p",null,"[2]"," J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei, ImageNet: A Large-Scale Hierarchical Image Database. IEEE Computer Vision and Pattern Recognition (CVPR), 2009. "),(0,i.kt)("p",null,"[3]"," Matek, C., Krappe, S., M\xfcnzenmayer, C., Haferlach, T., & Marr, C. (2021). An Expert-Annotated Dataset of Bone Marrow Cytology in Hematologic Malignancies ","[Data set]",". The Cancer Imaging Archive. ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.7937/TCIA.AXH3-T579"},"https://doi.org/10.7937/TCIA.AXH3-T579")),(0,i.kt)("p",null,"[4]"," Matek, C., Krappe, S., M\xfcnzenmayer, C., Haferlach, T., and Marr, C. (2021). Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a large image dataset. ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.1182/blood.2020010568"},"https://doi.org/10.1182/blood.2020010568")),(0,i.kt)("p",null,"[5]",' Samiksha Pachade, Prasanna Porwal, Dhanshree Thulkar, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, Luca Giancardo, Gwenol\xe9 Quellec, Fabrice M\xe9riaudeau, November 25, 2020, "Retinal Fundus Multi-disease Image Dataset (RFMiD)", IEEE Dataport, doi: ',(0,i.kt)("a",{parentName:"p",href:"https://dx.doi.org/10.21227/s3g7-st65"},"https://dx.doi.org/10.21227/s3g7-st65"),". "),(0,i.kt)("p",null,"[6]"," Derwin, D.J., Selvi, S.T. & Singh, O.J. Secondary Observer System for Detection of Microaneurysms in Fundus Images Using Texture Descriptors.\xa0J Digit Imaging\xa033,\xa0159\u2013167 (2020). ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.1007/s10278-019-00225-z"},"https://doi.org/10.1007/s10278-019-00225-z")),(0,i.kt)("p",null,"[7]"," Derwin, D.J., Selvi, S.T. & Singh, O.J. Discrimination of microaneurysm in color retinal images using texture descriptors.\xa0SIViP\xa014,\xa0369\u2013376 (2020). ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.1007/s11760-019-01566-6"},"https://doi.org/10.1007/s11760-019-01566-6")),(0,i.kt)("p",null,"[8]"," Jeba Derwin,D.,Tamil Selvi,S.,Jeba Singh,O.,Priestly Shan,B.,\u201d A novel automated system of discriminating Microaneurysms in fundus images\u201d, Biomedical Signal Processing and Control, Elsevier, Vol.58, 2020"),(0,i.kt)("p",null,"[9]"," David E, Madec S, Sadeghi-Tehran P, Aasen H, Zheng B, Liu S, Kirchgessner N, Ishikawa G, Nagasawa K, Badhon MA, Pozniak C. Global Wheat Head Detection (GWHD) dataset: a large and diverse dataset of high-resolution RGB-labelled images to develop and benchmark wheat head detection methods. Plant Phenomics. 2020 Aug 20;2020."),(0,i.kt)("p",null,"[10]"," Angel Cruz-Roa; Ajay Basavanhally; Fabio Gonz\xe1lez; Hannah Gilmore; Michael Feldman; Shridar Ganesan; Natalie Shih; John Tomaszewski; Anant Madabhushi. Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. Proc. SPIE 9041, Medical Imaging 2014: Digital Pathology, 904103 (20 March 2014);"),(0,i.kt)("p",null,"[11]"," Janowczyk A, Madabhushi A. Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. J Pathol Inform. 2016 Jul 26;7:29. doi: 10.4103/2153-3539.186902. PMID: 27563488; PMCID: PMC4977982."))}g.isMDXComponent=!0}}]);