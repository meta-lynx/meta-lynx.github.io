"use strict";(self.webpackChunkmetascatter=self.webpackChunkmetascatter||[]).push([[156],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return u}});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),f=c(n),u=o,m=f["".concat(s,".").concat(u)]||f[u]||p[u]||a;return n?r.createElement(m,i(i({ref:t},d),{},{components:n})):r.createElement(m,i({ref:t},d))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var c=2;c<a;c++)i[c]=n[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},7841:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return u},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return p}});var r=n(7462),o=n(3366),a=(n(7294),n(3905)),i=["components"],l={sidebar_position:3},s="Data Preparation: Object Detection",c={unversionedId:"getting-started/object-detection",id:"getting-started/object-detection",title:"Data Preparation: Object Detection",description:"In this section we describe how to create a CSV file from trained object detection tasks, which can be uploaded into Metascatter. We provide scripts for standard classification model architectures (with user-provided weights) for:",source:"@site/docs/getting-started/object-detection.md",sourceDirName:"getting-started",slug:"/getting-started/object-detection",permalink:"/metascatterV2/docs/getting-started/object-detection",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Data Preparation: Image Classification",permalink:"/metascatterV2/docs/getting-started/image-classification"},next:{title:"Data Preparation: Other",permalink:"/metascatterV2/docs/getting-started/data-preparation-v2"}},d={},p=[{value:"PyTorch",id:"pytorch",level:2},{value:"Downloads",id:"downloads",level:3},{value:"Quick Start",id:"quick-start",level:3},{value:"Prepare CSV file",id:"prepare-csv-file",level:3}],f={toc:p};function u(e){var t=e.components,l=(0,o.Z)(e,i);return(0,a.kt)("wrapper",(0,r.Z)({},f,l,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"data-preparation-object-detection"},"Data Preparation: Object Detection"),(0,a.kt)("p",null,"In this section we describe how to create a CSV file from trained object detection tasks, which can be uploaded into Metascatter. We provide scripts for standard classification model architectures (with user-provided weights) for:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"#pytorch"},"PyTorch"))),(0,a.kt)("p",null,"You will ",(0,a.kt)("strong",{parentName:"p"},"only need to edit some configuration files")," to point to your data and model weights."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Metalynx Object Detection",src:n(6769).Z,width:"1486",height:"1012"})),(0,a.kt)("h2",{id:"pytorch"},"PyTorch"),(0,a.kt)("h3",{id:"downloads"},"Downloads"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"PyTorch classifcation CSV creation script: ",(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1EFbPEJmq6vA3EHgi5HNsFfHOLdC8flGP/view?usp=sharing"},"Download")),(0,a.kt)("li",{parentName:"ul"},"Template configuration file: ",(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/138rJaJsS5_v-UDqP9JSCvcvHkoJBk61B/view?usp=sharing"},"Download")),(0,a.kt)("li",{parentName:"ul"},"Template transforms file: ",(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1FxvsQLbtKWYN5w1yCYcxNUC0LX0lcMzt/view?usp=sharing"},"Download")),(0,a.kt)("li",{parentName:"ul"},"Requirements file: ",(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1XRL_3RrTWnQLJmtRMENPVLiyWb7km4Et/view?usp=sharing"},"Download"))),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Usage: ",(0,a.kt)("inlineCode",{parentName:"strong"},"python create_csv_torch.py 'path_to_config_file.ini'"))),(0,a.kt)("h3",{id:"quick-start"},"Quick Start"),(0,a.kt)("p",null,"To create a CSV from a Pytorch object detection model, simply edit the variables in red in the template configuration file:\n",(0,a.kt)("img",{alt:"PyTorch Quickstart",src:n(4396).Z,width:"720",height:"547"})),(0,a.kt)("h3",{id:"prepare-csv-file"},"Prepare CSV file"),(0,a.kt)("p",null,"We provide scripts to create a CSV file that works with metascatter, given image folders and models. "),(0,a.kt)("p",null,"An example script for PyTorch classifcation models can be downloaded here: ",(0,a.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1EFbPEJmq6vA3EHgi5HNsFfHOLdC8flGP/view?usp=sharing"},"PyTorch script download"),". ",(0,a.kt)("em",{parentName:"p"},"You should not ordinarily need to edit this file.")," "),(0,a.kt)("p",null,"The following Python3 libraries are required:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"torch\ntorchvision\npillow>=9.1.0\npandas>=1.4.2\nsklearn\numap-learn\n")),(0,a.kt)("p",null,"The following models can be used with default or with your own trained weights:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'    "frcnn-resnet": torchvision.models.detection.fasterrcnn_resnet50_fpn\n    "frcnn-mobilenet-320": torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn\n    "frcnn-mobilenet": torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn\n    "retinanet": torchvision.models.detection.retinanet_resnet50_fpn\n    "fcos": torchvision.models.detection.fcos_resnet50_fpn\n    "ssd-vgg": torchvision.models.detection.ssd300_vgg16\n    "ssdlite": torchvision.models.detection.ssdlite320_mobilenet_v3_large\n')),(0,a.kt)("p",null,"You will need to supply a configuration file and a file describing the transforms for inference, for which templates can be found below:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/138rJaJsS5_v-UDqP9JSCvcvHkoJBk61B/view?usp=sharing"},"Template configuration file")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://drive.google.com/file/d/1FxvsQLbtKWYN5w1yCYcxNUC0LX0lcMzt/view?usp=sharing"},"Template transforms file"))),(0,a.kt)("p",null,"Usage: ",(0,a.kt)("strong",{parentName:"p"},(0,a.kt)("inlineCode",{parentName:"strong"},"python create_csv_OD_torch.py 'path_to_config_file.ini'"))),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"create_csv_OD_torch.py")," script should not be changed. Edit the configuration file as below."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[MODEL VARIABLES]\nmodel_name = frcnn-mobilenet-320\n# Choose from: frcnn-resnet, frcnn-mobilenet-320, frcnn-mobilenet, retinanet, fcos, ssd-vgg, ssdlite\nmodel_weights = /path/to/model/weight/file.pth\ntransform_name = inference \n# Should correspond to transforms_config.py\nconfidence_threshold = 0.5\nimage_size = 224\n")),(0,a.kt)("p",null,"Please use one of the standard model architectures listed above and provide the path to your trained weights. The ",(0,a.kt)("inlineCode",{parentName:"p"},"transform_name")," field should correspond to the name given in ",(0,a.kt)("inlineCode",{parentName:"p"},"transforms_config.py"),". "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[TRAIN IMAGE FOLDERS] \n# paths to labelled images and annotations\nimage_folder_paths = [/path/to/folder/of/labelled/images/1/ /path/to/folder/of/labelled/images/2/ /path/to/folder/of/labelled/images/3/]\nimage_folder_names = [Name_of_source_of_folder_1 Name_of_source_of_folder_2 Name_of_source_of_folder_3] \n# e.g. train and val folders For multiple locations, please separate folders and sources by a space.\n")),(0,a.kt)("p",null,"Inlcude a list of folders which store the labelled images you want to use. The folder structure of each image should be in the following class format:"),(0,a.kt)("p",null,"You can provide several folders, e.g. if you have different folders for ",(0,a.kt)("inlineCode",{parentName:"p"},"TRAINING")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"VALIDATION")," images. You can reference these by entering a corresponding name in the field ",(0,a.kt)("inlineCode",{parentName:"p"},"image_folder_names"),". Please ensure there are the same number of source names as folders provided. The folders and names should be separated by a ",(0,a.kt)("strong",{parentName:"p"},"space"),". "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[UNLABELLED FOLDERS]\nunlabelled_folder_paths = [/path/to/folder/of/unlabelled/images/1/ /path/to/folder/of/unlabelled/images/2/]\nunlabelled_folder_names = [Name_of_source_of_folder_1 Name_of_source_of_folder_2]\n# Unordered images: Folder->Image. For multiple locations, please separate folders and sources by a space.\n")),(0,a.kt)("p",null,"Similarly, you can include one or many folders for unlabelled test data. Leave blank if there are no such folders. "),(0,a.kt)("p",null,"In order to output class names (instead of numbers) to the CSV, you will need to provide a class list file."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[CLASS NAME FILE]\nclass_file = /path/to/file/with/class/names.txt\n")),(0,a.kt)("p",null,"The list of classes should be in order corresponding to the output of the model:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Class0\nClass1\nClass2\n...\nClassN\n")),(0,a.kt)("p",null,"Finally, enter the filename and path of the output ",(0,a.kt)("inlineCode",{parentName:"p"},"csv")," file and the folder to store the images with bounding box annotations drawn on them"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[OUTPUT FILENAME]\noutput_annotation_folder = /path/to/folder/of/images/with/bounding_boxes\nsavefile = /path/to/save/file.csv\n")),(0,a.kt)("p",null,"This works with standard architectures of the models named above with either default or trained weights. For bespoke architectures, please see ",(0,a.kt)("a",{parentName:"p",href:"#data-preparation"},"Data Preparation"),"."))}u.isMDXComponent=!0},6769:function(e,t,n){t.Z=n.p+"assets/images/metalynx_OD-730e0561932aa14c46f880ef82374bc2.png"},4396:function(e,t,n){t.Z=n.p+"assets/images/object_detection_config-7b07d354aaebfceff8077961c01edbc7.png"}}]);