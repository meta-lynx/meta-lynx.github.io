"use strict";(self.webpackChunkmetascatter=self.webpackChunkmetascatter||[]).push([[959],{3905:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return m}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(a),m=r,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||i;return a?n.createElement(h,o(o({ref:t},c),{},{components:a})):n.createElement(h,o({ref:t},c))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},8665:function(e,t,a){a.r(t),a.d(t,{assets:function(){return c},contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return d}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],s={sidebar_position:4},l="Data Preparation: Other",p={unversionedId:"getting-started/data-preparation-v2",id:"getting-started/data-preparation-v2",title:"Data Preparation: Other",description:"The input to Metascatter is a CSV file containing:",source:"@site/docs/getting-started/data-preparation-v2.md",sourceDirName:"getting-started",slug:"/getting-started/data-preparation-v2",permalink:"/metascatterV2/docs/getting-started/data-preparation-v2",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Data Preparation: Object Detection",permalink:"/metascatterV2/docs/getting-started/object-detection"},next:{title:"Data Loading",permalink:"/metascatterV2/docs/getting-started/data-loading"}},c={},d=[{value:"Creating the Image Co-ordinates",id:"creating-the-image-co-ordinates",level:2},{value:"Feature extraction",id:"feature-extraction",level:3},{value:"<strong>Classification</strong>",id:"classification",level:4},{value:"<strong>Segmentation (Keras)</strong>",id:"segmentation-keras",level:4},{value:"<strong>Object Detection (PyTorch)</strong>",id:"object-detection-pytorch",level:4},{value:"<strong>No model</strong>",id:"no-model",level:4},{value:"Dimensionality reduction",id:"dimensionality-reduction",level:2}],u={toc:d};function m(e){var t=e.components,s=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"data-preparation-other"},"Data Preparation: Other"),(0,i.kt)("p",null,"The input to Metascatter is a ",(0,i.kt)("strong",{parentName:"p"},"CSV file")," containing:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Image paths")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"2D co-ordinate")," representations of the images (obtained using dimensionality reduction on raw images or model features)"),(0,i.kt)("li",{parentName:"ul"},"Associated ",(0,i.kt)("strong",{parentName:"li"},"metadata"))),(0,i.kt)("p",null,"For instance, below is an example CSV for the dog breed classification example:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Dog breed classification CSV",src:a(494).Z,width:"1667",height:"161"}),"\nIn this case, the ",(0,i.kt)("inlineCode",{parentName:"p"},"Paths")," column contains the relative paths to the images (you will be able to specific the path prefix when loading the csv). You could also use the full paths to an accessible online storage. ",(0,i.kt)("inlineCode",{parentName:"p"},"X-Coord")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Y-Coord")," provide the co-ordinates for the scatterplot. These three fields are the minimum needed for metascatter to run. "),(0,i.kt)("p",null,"Other columns of the CSV can contain any other type of metadata. For instance, this could be data output from the model, e.g. predictions and confidence of predictions, or image information such as ground truth labels, acquisition method, associated demographic data and so forth. The more metadata, the more analyses you can do in ",(0,i.kt)("inlineCode",{parentName:"p"},"Metascatter"),"."),(0,i.kt)("p",null,"For ",(0,i.kt)("strong",{parentName:"p"},"image classification")," tasks, you can produce an example CSV file using the scripts provided in ",(0,i.kt)("a",{parentName:"p",href:"/metascatterV2/docs/getting-started/image-classification"},"Image Classification"),"."),(0,i.kt)("h2",{id:"creating-the-image-co-ordinates"},"Creating the Image Co-ordinates"),(0,i.kt)("p",null,"In this section we will discuss how to create the 2D co-ordinate representations for each image. this involves:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"#feature-extraction"},"Feature extraction"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("a",{parentName:"p",href:"#dimensionality-reduction"},"Dimensionality reduction")),(0,i.kt)("p",{parentName:"li"},"The co-ordinates can be obtained by either using image intensities or ",(0,i.kt)("em",{parentName:"p"},"features")," extracted by passing the images through a model as in ",(0,i.kt)("a",{parentName:"p",href:"#feature-extraction"},"Feature Extraction"),". After extracting either the intensity or model features, these will have to be reduced to two-dimensional co-ordinates as described in ",(0,i.kt)("a",{parentName:"p",href:"#dimensionality-reduction"},"Dimensionality Reduction"),"."))),(0,i.kt)("h3",{id:"feature-extraction"},"Feature extraction"),(0,i.kt)("p",null,"Machine learning methods typically work by transforming the input data to a reduced number of features on which the task (e.g. classification, object detection, segmentation) can work more succesfully. For instance, for a classifier that distinguishes between images of zebras and horses, features representing a stripy pattern would work better than features representing legs and tails."),(0,i.kt)("p",null,"In deep learning, these features are optimised in the training process (rather than being pre-defined as in conventional machine learning techqniques). ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("inlineCode",{parentName:"strong"},"Metascatter"))," allows you to evaluate the performance of a deep learning model by visualising how the model 'sees' the images in the dataset in terms of these features. For a well-performing model, you would expect a clear boundary between the features of horses and the features of zebras. "),(0,i.kt)("p",null,"Different layers of a trained deep learning model will produce different features. You can obtain these features by running each image through the model up to the relevant layer. "),(0,i.kt)("p",null,"For example, for classification models such as VGG16, we can pass the images through a feature model up to the layer before the final classification: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"(classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n================================================================\n    # (4): ReLU(inplace=True)\n    # (5): Dropout(p=0.5, inplace=False)\n    # (6): Linear(in_features=4096, out_features=1000, bias=True)\n)\n")),(0,i.kt)("p",null,"Some scripts for doing this with common example models are given below:"),(0,i.kt)("h4",{id:"classification"},(0,i.kt)("strong",{parentName:"h4"},"Classification")),(0,i.kt)("p",null,"Multi-class classification (120 classes) using ",(0,i.kt)("inlineCode",{parentName:"p"},"PyTorch Mobilenet"),". We recommend using the last layer before the classification head. ",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/17BoNA_gD7kpJ1ZO8Z0bUJGD2SugAoOJe/view?usp=sharing"},"[Download script here]")),(0,i.kt)("h4",{id:"segmentation-keras"},(0,i.kt)("strong",{parentName:"h4"},"Segmentation (Keras)")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"U-Net")," for pixel-level segmentation of images of small animals.  Download: ",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1AVZhp5xeIEaLaowvPxn3zic1t9iB02fb/view?usp=sharing"},"[Script]"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1FMrkt7zCSX2FYtEX6jRnIJI6cpFyeXFI/view?usp=sharing"},"[Model]"),"\n",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/uc?export=download&id=1bQk4icaenSUbvqd2gAjd2XkyfFPz-T6_&ndplr=1"},"[Data]")),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip ")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"For a deep learning segmentation model, we typically choose the output features after the final encoding layer (before any upsampling)."))),(0,i.kt)("p",null,"In ",(0,i.kt)("inlineCode",{parentName:"p"},"keras"),", you can output the features by creating a sub-model up to the required named layer:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"model = unet()\nmodel.load_weights('my_trained_model.h5')\nsubmodel = Model(model.inputs, model.get_layer('conv2d_transpose_1').output)\nfeatures = submodel.predict(np.expand_dims(img,0))\n")),(0,i.kt)("h4",{id:"object-detection-pytorch"},(0,i.kt)("strong",{parentName:"h4"},"Object Detection (PyTorch)")),(0,i.kt)("p",null,"Object detection using Ultralytics ",(0,i.kt)("inlineCode",{parentName:"p"},"YOLOv5 small")," ",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1LkZ4jfU9VC-LxPbW5dtqCwBBHaG-c34g/view?usp=sharing"},"[Download script here]")," ",(0,i.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1pk2Jz2dd65_qGJP1nJwQzsRZPq70Eaqk/view?usp=sharing"},"[Example CSV here]")),(0,i.kt)("h4",{id:"no-model"},(0,i.kt)("strong",{parentName:"h4"},"No model")),(0,i.kt)("p",null,"If you do not have a trained model, you can use the raw image (or after some pre-processing) intensities as features instead."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"features = img.flatten()\n")),(0,i.kt)("h2",{id:"dimensionality-reduction"},"Dimensionality reduction"),(0,i.kt)("p",null,"The resulting data (features or image intensities) might still be several thousands long for each image. For instance, in the above VGG16 classification model, 4096 features are extracted from layer (3). To visualise these on a 2D plot, we need to reduce the dimensionality while preserving the structure of relationships between data points. "),(0,i.kt)("p",null,"We therefore apply ",(0,i.kt)("strong",{parentName:"p"},"dimensionality reduction")," to these features. We use ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"},(0,i.kt)("inlineCode",{parentName:"a"},"T-SNE"))," to obtain a 2D representation of the data that compactly describes the underlying structure. In ",(0,i.kt)("inlineCode",{parentName:"p"},"python"),", you can use the ",(0,i.kt)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htm"},(0,i.kt)("inlineCode",{parentName:"a"},"sklearn"))," library."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\nX_coords = tsne.fit_transform(features)\n")))}m.isMDXComponent=!0},494:function(e,t,a){t.Z=a.p+"assets/images/example_csv-5e3aa18449b1483df7648711cd789c93.png"}}]);